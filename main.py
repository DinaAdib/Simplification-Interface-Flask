from flask import Flask, request, render_template, jsonify
import os
# from initializeLS import *
app = Flask(__name__)

LS_path = '~/PycharmProjects/FinalLS/'

global substitutions_db
global fivegram_model
global threegram_model
global syllable_dict
global subs_rank_nnclf
global wiki_frequency

def init():
    global substitutions_db
    substitutions_db = load_obj(DIRECTORY + "substitutions")
    global fivegram_model
    global threegram_model
    global syllable_dict
    global subs_rank_nnclf
    fivegram_model = NgramModel(DIRECTORY + "newsela.lm", 2, 2)
    threegram_model = NgramModel(DIRECTORY + "newsela.lm", 1, 1)
    syllable_dict = pyphen.Pyphen(lang='en')
    # Load the subs ranking file
    subs_rank_nnclf = load_obj(DIRECTORY + 'subs_ranking_model_3')
    global wiki_frequency
    wiki_frequency = WikiFrequency()



# -*- coding: utf-8 -*-
"""meowLS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bc7GAqq50ZsW_8zQQ8PNje1wJoIodFcS
"""

from functions import *
from Substitution_Generator import *
from Features.WikiFrequency import *
from Features.Ngram import *
dic = pyphen.Pyphen(lang='en')
# """**lexicon**"""

lexicons = {}
with open(DIRECTORY+"/lexicon.tsv") as f:
    for line in f:
        (key, val) = line.split()
        lexicons[key.lower()] = val
# # """**Model**"""
# #
# # from sklearn.preprocessing import maxabs_scale
# # import re
# #
#
FEATURES_COUNT= 8
# print("loaded models")

# scaler = StandardScaler()
# myword2vec = Word2Vec()
# print("loaded word2vec")
# np.save("word2vec", myword2vec)
def get_similarity_scores(candidates , complex_word):
  # complex_vec = myword2vec.get_vector(complex_word)
  scores = []
  for candidate in candidates :
      scores.append(0.0)
    # candidate_vec = myword2vec.get_vector(candidate)
    # scores.append(myword2vec.get_cosine_similarity([candidate_vec], [complex_vec]) )
  return maxabs_scale(scores)

def get_ngram_scores(candidates , complex_word , sentence , model):
  scores = []
  sentence = sentence.lower()
  complex_word = re.sub(r'[^a-zA-Z0-9\s]', ' ', complex_word)
  for candidate in candidates:
    candidate = re.sub(r'[^a-zA-Z0-9\s]', ' ', candidate)
    scores.append(model.get_score(sentence.split(), complex_word.lower(), candidate.lower()))
#
  #print(scores)
  return maxabs_scale(scores)

def get_lexicon_scores(candidates):
  scores = []
  for candidate in candidates:
    if candidate.lower() in lexicons:
      scores.append( float(lexicons[candidate.lower()]) )
    else :
      scores.append(2.5) ## average word
  return maxabs_scale(scores)

def get_syllable_counts(candidates):
  scores = []
  for candidate in candidates:
    scores.append(syllable_dict.inserted(candidate).count('-') + 1)
  return maxabs_scale(scores)

def get_character_counts(candidates):
  scores = []
  for candidate in candidates:
    scores.append( len(candidate) )
  return maxabs_scale(scores)

def get_frequencies(candidates):
  scores = []
  for candidate in candidates:
    scores.append( wiki_frequency.get_feature( candidate.lower() ) )
  return maxabs_scale(scores)


def get_wiki_frequencies(candidates):
  scores = []
  for candidate in candidates:
    scores.append( zipf_frequency(candidate.lower(),  'en') )
  return maxabs_scale(scores)


def get_features(filename , train= True):

  all_features = np.zeros(FEATURES_COUNT)
  all_ranks = []
  all_lines = []
  all_words = []
  with open(DIRECTORY+filename) as file:
      corpus = file.read()
      lines= corpus.split("\n")
      increment = 0
      for line in lines :
        try:
          increment += 1
          #print(line)
          ### Processing line
          tokens = line.strip().split('\t')
          sentence = tokens[0].strip()
          complex_word = tokens[1].strip()
          #print(complex_word)
          ranks = [int(token.strip().split(':')[0]) for token in tokens[3:]]
          if train :
            candidates = [token.strip().split(':')[1] for token in tokens[3:]]
          else:
            candidates = get_candidates(complex_word , substitutions_db)
          #print(candidates)

          ### Extracting Features
          cosine_similarities = get_similarity_scores(candidates , complex_word)
          fivegram_scores = get_ngram_scores(candidates , complex_word , sentence , fivegram_model)
          threegram_scores = get_ngram_scores(candidates , complex_word , sentence , threegram_model)
          lexicon_scores = get_lexicon_scores(candidates)
          syllables = get_syllable_counts( candidates )
          characters = get_character_counts( candidates)
          frequencies = get_frequencies( candidates)
          wiki_frequencies = get_wiki_frequencies ( candidates)
        except:
          print("************** BIG ERROR OCCURED")
          continue

        ## No error so we will append
        for i in range(len(candidates)) :
          all_lines.append(sentence)
          all_words.append(candidates[i])
        features = np.column_stack( (cosine_similarities , fivegram_scores , threegram_scores, lexicon_scores ,syllables , characters , frequencies , wiki_frequencies ) )
        all_ranks.extend(ranks)
        all_features= np.vstack((all_features , features))

        if increment > 40000:
          print("lines", len(all_lines))
          print("features: ", all_features.shape)
          print("finisheeeedd ",increment)
          increment = 0
          np.save(DIRECTORY+"_meow_ftrs"+str(increment) , all_features[1:len(all_features)])
          np.save(DIRECTORY+"_meow_ranks"+str(increment) , all_ranks)
          np.save(DIRECTORY+"_meow_lines"+str(increment) , all_lines)
          np.save(DIRECTORY+"_meow_words"+str(increment) , all_words)



      print("finallly " , np.array(all_features).shape)
      return all_features[1:len(all_features)] , all_ranks , all_lines , all_words


"""**Classifier**"""

"""# Full Model"""

def get_line_features(line ):
  try:
  #print(line)
    ### Processing line
    tokens = line.strip().split('\t')
    sentence = tokens[0].strip()
    complex_word = tokens[1].strip()
    #print(complex_word)
    ranks = [int(token.strip().split(':')[0]) for token in tokens[3:]]
    candidates = [token.strip().split(':')[1] for token in tokens[3:]]
    our_candidates = get_candidates(complex_word , substitutions_db)
    #print(candidates)

    ### Extracting Features
    cosine_similarities = get_similarity_scores(candidates , complex_word)
    fivegram_scores = get_ngram_scores(candidates , complex_word , sentence , fivegram_model)
    threegram_scores = get_ngram_scores(candidates , complex_word , sentence , threegram_model)
    lexicon_scores = get_lexicon_scores(candidates)
    syllables = get_syllable_counts( candidates )
    characters = get_character_counts( candidates)
    frequencies = get_frequencies( candidates)
    wiki_frequencies = get_wiki_frequencies ( candidates)
  except:
    print("************** BIG ERROR OCCURED")
    return False, None , None , None , None , None

  ## No error
  features = np.column_stack( (cosine_similarities , fivegram_scores , threegram_scores, lexicon_scores ,syllables , characters , frequencies , wiki_frequencies ) )

  return True , features , our_candidates , candidates, ranks , complex_word


def preprocess_line(line):
  try:
    tokens = line.strip().split('\t')
    sentence = tokens[0].strip()
    sentence =  re.sub(r'[^a-zA-Z0-9\s]', ' ', sentence )
    complex_word = tokens[1].strip()
    #print(complex_word)
    ranks = [int(token.strip().split(':')[0]) for token in tokens[3:]]
    candidates = [token.strip().split(':')[1] for token in tokens[3:]]
  except:
    print("SOMETHING WRING ")
    return None , None , None , None
  return sentence, candidates, ranks , complex_word

def preprocess_interface_line(line):
  try:
    # sentence = line.strip().split('')
    # print("SENTENCE ",sentence)
    # sentence = tokens[0].strip()
    sentence =  re.sub(r'[^a-zA-Z0-9\s]', ' ', line )
    complex_word = ""
    #print(complex_word)
    ranks = []
    candidates = []
  except:
    print("SOMETHING WRING ")
    return None , None , None , None
  return sentence, candidates, ranks , complex_word

def get_candidates_features(candidates , sentence , complex_word):
  try:
    cosine_similarities = get_similarity_scores(candidates , complex_word)
    fivegram_scores = get_ngram_scores(candidates , complex_word , sentence , fivegram_model)
    threegram_scores = get_ngram_scores(candidates , complex_word , sentence , threegram_model)
    lexicon_scores = get_lexicon_scores(candidates)
    syllables = get_syllable_counts( candidates )
    characters = get_character_counts( candidates)
    frequencies = get_frequencies( candidates)
    wiki_frequencies = get_wiki_frequencies ( candidates)
  except:
    print("************** BIG ERROR OCCURED")
    return False, None ,

  ## No error
  features = np.column_stack( (cosine_similarities , fivegram_scores , threegram_scores, lexicon_scores ,syllables , characters , frequencies , wiki_frequencies ) )

  return True , features

# from sklearn.model_selection import train_test_split
# from sklearn.neural_network import MLPClassifier
# from scipy.stats import pearsonr
# from sklearn.preprocessing import StandardScaler


total_count =  0
total_words_count = 0
ratio = 0
pos_ratio = 0
hit = 0
pos_hit = 0
all_ratios = []


def rank(input_path):
    print("RANKING ")
    word_hit = 0,
    test_lines = open(input_path).readlines()
    for line in test_lines:
        print("Line is ", line)
        sentence, candidates, ranks, complex_word = preprocess_interface_line(line)
        # print(sentence)
        print(complex_word)
        if sentence is not None:
            original = sentence
            for w in sentence.split():
                if (w in lexicons and float(lexicons[w]) > 3.0) or zipf_frequency(w, 'en') <= 4:
                    # print(w , "freq  ", zipf_frequency(w , 'en'))

                    complex_lm_score = fivegram_model.evaluate_context(sentence, w)
                    word_hit += 1

                    candidates = get_candidates(w, substitutions_db)

                    candidates_pos = list(convert_postag(w, candidates))

                    indicator, candidates_features = get_candidates_features(candidates_pos, sentence, w)

                    # print(np.array(candidates_features).shape)
                    if indicator == True:
                        candidate_predictions = subs_rank_nnclf.predict(candidates_features)
                        #         best_candidates =candidates_pos[np.argmin(candidate_predictions)]  B3DEEEN
                        #         print(best_candidates)
                        min_frequency = 10
                        best_of_best = w
                        min_context = complex_lm_score
                        for cindex, best in enumerate(candidates_pos):
                            if candidate_predictions[cindex] == min(candidate_predictions):
                                freq_diff = zipf_frequency(best, 'en') - zipf_frequency(w, 'en')
                                word_lm_score = fivegram_model.evaluate_context(sentence.replace(w, best), best)
                                if freq_diff > 0 and freq_diff < min_frequency and word_lm_score > min_context:
                                    min_frquency = freq_diff
                                    best_of_best = best
                                    # print("best of best updated with", best_of_best)
                        sentence = sentence.replace(w, best_of_best)
            # for our_complex, our_simple in replacements.items():
            #     sentence = sentence.replace(our_complex, our_simple)
            # print(" sentence simplified: " , sentence.replace(w, best_of_best ) )
            with open("model/output.txt", "w+") as f:
                # f.write("<original> ")
                # f.write(original)
                # f.write("\n<simplified> ")
                f.write(sentence)
                f.write("\n")
                f.close()




@app.route('/', methods=['GET', 'POST'])
def index():
    text_input = ""
    context = {}

    lines = ""
    if request:
        print("request")
        Sentence_Simplifier = request.args.get('SSbtn')
        Sentence_Summarizer = request.args.get('summarizebtn')
        Lexical_Simplifier = request.args.get('LSbtn')
        print(Sentence_Simplifier)
        if Sentence_Simplifier or Sentence_Summarizer or Lexical_Simplifier:

            text_input = request.args.get('text_input')
            if text_input is not None:
                if Sentence_Simplifier == 'Sentence Simplification':
                    # execute this code
                    print("I am here")

                    f = open('model/input.txt', 'w')
                    f.write(text_input)
                    f.close()
                    command = 'python model/nmt/translate.py -model model/_step_8901.pt \
                                            -src model/input.txt\
                                            -output model/output.txt \
                                            -replace_unk \
                                            -beam_size 5'
                    print("text line ",text_input)
                    os.system(command)

                elif Sentence_Summarizer == 'Summarize':

                    text_input = request.args.get('text_input')
                    if text_input is not None:
                        f = open('model/input.txt', 'w')
                        f.write(text_input)
                        f.close()
                    print("Input line is ", text_input)
                    command = 'python3 model/Summarizer.py model/output.txt 0.4 model/input.txt'

                    os.system(command)
                elif Lexical_Simplifier == 'Lexical Simplification':
                    text_input = request.args.get('text_input')
                    if text_input is not None:
                        f = open('model/input.txt', 'w')
                        f.write(text_input)
                        f.close()
                    # command = 'python3 meowls.py ' + 'model/output.txt ' + 'model/input.txt'
                    # os.system(command)
                    rank('model/input.txt')

            with open('model/output.txt', 'r') as f:
                for line in f.readlines():
                    print(line)
                    lines += line

                f.close()
            # context = {text_input: text_input, text_output': lines}
    return render_template("index.html",text_input= text_input,  text_output= lines)

if __name__ == "__main__":
    # substitutions_db = load_obj(DIRECTORY + "substitutions")
    # fivegram_model = NgramModel(DIRECTORY + "newsela.lm", 2, 2)
    # threegram_model = NgramModel(DIRECTORY + "newsela.lm", 1, 1)
    # self.syllable_dict = pyphen.Pyphen(lang='en')
    # self.wiki_frequency = WikiFrequency()
    init()
    app.run()



# if __name__ == "__main__":
    # outputFile = open(str(sys.argv[1]), 'w+')
    # inputPath = str(sys.argv[2])
    # start_time = time.time()
    # rank(str(inputPath))
    # elapsed_time = time.time() - start_time
    # print("Elapsed time = ", elapsed_time)
    # outputFile.write(output)
    # rank(DIRECTORY+'BenchLS.test.txt')

    # input_word = "dream"
    #
    # new_instance = Thesaurus(input_word)

    # Get the synonyms according to part of speech
    # Default part of speech is noun

    # print(new_instance.get_synonym())
